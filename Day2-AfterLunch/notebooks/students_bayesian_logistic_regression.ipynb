{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PGM-Lab/2022-ProbAI/blob/main/Day2-AfterLunch/notebooks/students_bayesian_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXcRh2TQfyhp"
      },
      "source": [
        "## Setup\n",
        "Let's begin by installing and importing the modules we'll need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EggRgZ1gfyhq"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyro-ppl torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import types\n",
        "import matplotlib.pyplot as plt\n",
        "from pyro.infer import Predictive\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Uniform, Delta, Gamma, Binomial\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "import torch.distributions.constraints as constraints\n",
        "import pyro.optim as optim\n",
        "from pyro.contrib.autoguide import AutoNormal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# for CI testing\n",
        "pyro.set_rng_seed(1)\n",
        "pyro.enable_validation(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtC9nacRfyhq"
      },
      "source": [
        "# Dataset \n",
        "\n",
        "The following example is taken from \\[1\\].  We would like to explore the relationship between topographic heterogeneity of a nation as measured by the Terrain Ruggedness Index (variable *rugged* in the dataset) and its GDP per capita. In particular, it was noted by the authors in \\[1\\] that terrain ruggedness or bad geography is related to poorer economic performance outside of Africa, but rugged terrains have had a reverse effect on income for African nations. Let us look at the data \\[2\\] and investigate this relationship.  We will be focusing on three features from the dataset:\n",
        "  - `cont_africa`: whether the given nation is in Africa\n",
        "  - `rugged`: quantifies the Terrain Ruggedness Index\n",
        "  - `rgdppc_2000`: Real GDP per capita for the year 2000\n",
        " \n",
        "  \n",
        "We will take the logarithm for the response variable GDP as it tends to vary exponentially. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akcHul9xfyhr"
      },
      "outputs": [],
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/pyro-ppl/brmp/master/brmp/examples/rugged_data.csv\"\n",
        "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
        "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
        "df = df[np.isfinite(df.rgdppc_2000)]\n",
        "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
        "\n",
        "data = torch.tensor(df.values, dtype=torch.float)\n",
        "x_data, y_data = data[:, (1,2)], data[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjOGg_Dafyhu"
      },
      "outputs": [],
      "source": [
        "# Display first 10 entries \n",
        "display(df[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itLI29xgqkaV"
      },
      "outputs": [],
      "source": [
        "def prepare_figure(title='Scatter plot of data', x_data_ = None, y_data_ = None):\n",
        "    \"\"\"\n",
        "    Plot the data and return the figure axis for possible subsequent additional plotting.\n",
        "    :param title: Title of the plot\n",
        "    :param x_data_: Nx2 numpy array or torch tensor\n",
        "    :param y_data_: Nx1 numpy array or torch tensor with the class labels.\n",
        "    :return: Figure axis.\n",
        "    \"\"\"\n",
        "    if x_data_ is None and y_data_ is None:\n",
        "        x_data_ = x_data\n",
        "        y_data_ = y_data\n",
        "\n",
        "    if type(x_data_) is torch.Tensor:\n",
        "        x_data_ = x_data_.numpy()\n",
        "        y_data_ = y_data_.numpy()\n",
        "\n",
        "    xx, yy = np.mgrid[np.floor(np.min(x_data_[:, 0])):np.ceil(np.max(x_data_[:, 0])):.01,\n",
        "             np.floor(np.min(x_data_[:, 1])):np.ceil(np.max(x_data_[:, 1])):.01]\n",
        "\n",
        "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "\n",
        "    f, ax = plt.subplots(figsize=(8, 6))\n",
        "    f.suptitle(title, fontsize=16)\n",
        "\n",
        "    ax.scatter(x_data[y_data_==0,0], x_data[y_data_==0, 1], c='g', s=50,\n",
        "               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "               edgecolor=\"white\", linewidth=1, label='Non-African')\n",
        "\n",
        "    ax.scatter(x_data[y_data_==1,0], x_data[y_data_==1, 1], c='orange', s=50,\n",
        "               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
        "               edgecolor=\"white\", linewidth=1, label='African')\n",
        "\n",
        "    ax.set(aspect=\"equal\",\n",
        "           xlim=(0, 7), ylim=(6, 11),\n",
        "           xlabel=\"Rugged\", ylabel=\"Log GDP\")\n",
        "\n",
        "    ax.legend()\n",
        "\n",
        "    return ax, grid, xx, yy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTWhdZIqkaW"
      },
      "outputs": [],
      "source": [
        "prepare_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFJeNXiUfyhx"
      },
      "source": [
        "# 1. Logistic Regression\n",
        "\n",
        "Logistic Regression is one of the most commonly used supervised learning tasksin machine learning. Suppose we're given a dataset $\\mathcal{D}$ of the form\n",
        "\n",
        "$$ \\mathcal{D}  = \\{ ({\\bf x_i}, y_i) \\} \\qquad \\text{for}\\qquad i=1,2,...,N$$\n",
        "\n",
        "where ${\\bf X_i}\\in {\\mathbb R}^m$ and $y_i\\in \\{0,1\\}$.\n",
        "\n",
        "The goal of logistic regression is to fit a model that correctly predicts the probabilities of the class labels:\n",
        "\n",
        "$$ p(y|x) = \\frac{1}{1+e^{-b  -{\\bf w}^T {\\bf x} }}$$\n",
        "\n",
        "where ${\\bf w}$ and $b$ are learnable parameters. Specifically $w$ is a vector of weights and $b$ is a bias term.\n",
        "\n",
        "First we implement a logistic regression model in PyTorch and learn point estimates for the parameters ${\\bf w}$ and $b$.  Afterwards we'll see how to incorporate uncertainty into our estimates by using Pyro to doing Bayesian logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHG9QQYhfyhy"
      },
      "source": [
        "## 1.1 Model\n",
        "Using a logistic regresison model, we want to predict whether a nation is african or not as a function of the terrain rugedness index and log GDP per capita of a nation.  \n",
        "\n",
        "Our input `x_data` is a tensor of size $N \\times 2$ and our output `y_data` is a tensor of size $N \\times 1$.  The method `predict(self,x_data)` defines a sigmoid transformation of the form $\\mathit{sigmoid}({\\bf x}^T{\\bf w} + b)$, where ${\\bf w}$ is the weight vector and $b$ is the additive bias.\n",
        "\n",
        "The parameters of the model are defined using ``torch.nn.Parameter``, and will be learned during training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBQBgFkPfyhz"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionModel():\n",
        "    def __init__(self):\n",
        "        self.w = torch.nn.Parameter(torch.zeros(1, 2))\n",
        "        self.b = torch.nn.Parameter(torch.zeros(1, 1))\n",
        "\n",
        "    def params(self):\n",
        "        return {\"b\":self.b, \"w\": self.w}\n",
        "\n",
        "    def predict(self, x_data):\n",
        "        return torch.sigmoid(-self.b - torch.mm(self.w, torch.t(x_data))).squeeze(0)\n",
        "\n",
        "    def logits(self, x_data):\n",
        "        return (self.b + torch.mm(self.w, torch.t(x_data))).squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h80i4g8xqkaY"
      },
      "outputs": [],
      "source": [
        "logistic_regression_model = LogisticRegressionModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlGU_7YPfyhz"
      },
      "source": [
        "## 1.2 Training\n",
        "For training we will use the cross entropy as our loss and Adam as our optimizer. We will use a somewhat large learning rate of `0.05` and run for 1000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N6WPDJufyh0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train(num_iterations = 1000):\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "    optim = torch.optim.Adam(logistic_regression_model.params().values(), lr=0.05)\n",
        "\n",
        "    for j in range(num_iterations):\n",
        "        # run the model forward on the data\n",
        "        logits = logistic_regression_model.logits(x_data)\n",
        "        # calculate the cross-entropy loss\n",
        "        loss = loss_fn(logits,y_data)\n",
        "        # initialize gradients to zero\n",
        "        optim.zero_grad()\n",
        "        # backpropagate\n",
        "        loss.backward()\n",
        "        # take a gradient step\n",
        "        optim.step()\n",
        "        if (j + 1) % 500 == 0:\n",
        "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
        "    # Inspect learned parameters\n",
        "    print(\"Learned parameters:\")\n",
        "    for name, param in logistic_regression_model.params().items():\n",
        "        print(name, param.data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_F9KDFyqkaZ"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyfAY0h-fyh0"
      },
      "source": [
        "## 1.3 Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmgazCZJfyh1"
      },
      "source": [
        "We now plot the decision line learned for african and non-afrian nations relating the rugeedness index with the GDP of the country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWUs5dc1fyh1"
      },
      "outputs": [],
      "source": [
        "ax, grid, xx, yy = prepare_figure('Decision line')\n",
        "probs = logistic_regression_model.predict(grid).reshape(xx.shape).detach().numpy()\n",
        "ax.contour(xx, yy, probs, levels=[.5], cmap=\"Reds\", vmin=0, vmax=.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yrEaqT5fyh3"
      },
      "source": [
        "# 2. Bayesian Logistic Regression\n",
        "\n",
        "\n",
        "[Bayesian modeling](http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf) offers a systematic framework for reasoning about model uncertainty. Instead of just learning point estimates, we're going to learn a _distribution_ over variables that are consistent with the observed data.\n",
        "\n",
        "In order to make our linear regression Bayesian, we need to put priors on the parameters ${\\bf w}$ and $b$. These are distributions that represent our prior belief about reasonable values for $\\{bf w}$ and ${\\bf b}$ (before observing any data).\n",
        "\n",
        "A graphical representation would be as follows:\n",
        "\n",
        "<img src=\"https://github.com/PGM-Lab/probai-2021-pyro/raw/main/Day3/Figures/BayesianLogisticRegressionPGM.png\" width=800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kltwl1J9fyh3"
      },
      "source": [
        "## 2.1 Model\n",
        "\n",
        "We now have all the ingredients needed to specify our model. First we define priors over weights and bias. The prior on the intercept parameter is very flat as we would like this to be learnt from the data. We are using a weakly regularizing prior on the regression coefficients to avoid overfitting to the data.\n",
        "\n",
        "We use the `obs` argument to the `pyro.sample` statement to condition on the observed data `y_data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUWfkBXUfyh3"
      },
      "source": [
        "### <span style=\"color:red\">Exercise </span> \n",
        " \n",
        "* Define a random variable \"b\" to model the intercept. \n",
        "* Define the class random variable \"african/non-african\" for the predicited labels.\n",
        "* This random variable is defined as Binomial distribution and is parametrized with the logits. \n",
        "* If time permits, explore and experiment with the notebook; e.g., specification of prior distributions, manually specified guides, and modifications to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_19buBJsfyh4"
      },
      "outputs": [],
      "source": [
        "def model(x_data, y_data):\n",
        "    # weight and bias priors\n",
        "    with pyro.plate(\"plate_w\", 2):\n",
        "        w = pyro.sample(\"w\", Normal(torch.zeros(1,1), torch.ones(1,1)))\n",
        "\n",
        "    # Define a random variable \"b\" to model the intercept.\n",
        "   \n",
        "\n",
        "    with pyro.plate(\"map\", len(x_data)):\n",
        "        # Compute logits (i.e. log p(x=0)/p(x=1)) as a linear combination between data and weights.\n",
        "        logits = (b + torch.mm(x_data,torch.t(w))).squeeze(-1)\n",
        "        # Define a Binomial distribution as the observed value parameterized by the logits.\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcu6i1mYfyh6"
      },
      "source": [
        "## 2.2 Guide\n",
        "\n",
        "In order to do inference we're going to need a guide, i.e. a variational family of distributions.  We will use Pyro's [autoguide library](https://docs.pyro.ai/en/stable/infer.autoguide.html). Under the hood, this defines a `guide` function, which in this case provides us with `Normal` variation distributions with learnable parameters, one for each sample `sample()` statement in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcC93Jkfyh7"
      },
      "outputs": [],
      "source": [
        "guide = AutoNormal(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktwd6CCUfyh8"
      },
      "source": [
        "## 2.3 Inference\n",
        "\n",
        "To do inference we'll use stochastic variational inference (SVI). Just like in the non-Bayesian linear regression, each iteration of our training loop will take a gradient step, but now we will use the ELBO objective instead of binary cross entropy by constructing a `Trace_ELBO` object that we pass to `SVI`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py-1QUeyfyh9"
      },
      "source": [
        "To take an ELBO gradient step we simply call the step method of SVI. Notice that the data argument we pass to step will be passed to both model() and guide().  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wh1Tyqjfyh9"
      },
      "outputs": [],
      "source": [
        "def train_vi(x_data, y_data, model, guide=None, num_iterations = 1000):\n",
        "    optim = Adam({\"lr\": 0.1})\n",
        "\n",
        "    # if no guide is provided, resort to an autoguide\n",
        "    guide_ = guide if guide is not None else AutoNormal(model)\n",
        "\n",
        "    svi = SVI(model, guide_, optim, loss=Trace_ELBO(), num_samples=10)\n",
        "\n",
        "    pyro.clear_param_store()\n",
        "    for j in range(num_iterations):\n",
        "        # calculate the loss and take a gradient step\n",
        "        loss = svi.step(x_data, y_data)\n",
        "        if j % 500 == 0:\n",
        "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_7bGZH8qkac"
      },
      "source": [
        "Learn the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r11Qa4DSqkac"
      },
      "outputs": [],
      "source": [
        "guide = AutoNormal(model)\n",
        "train_vi(x_data, y_data, model, guide=guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8GLwJV6qkac"
      },
      "source": [
        "Get the learned parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfaPyhfTfyh9"
      },
      "outputs": [],
      "source": [
        "for name, value in pyro.get_param_store().items():\n",
        "    print(name, pyro.param(name).data.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKfYcpnYfyh-"
      },
      "source": [
        "As you can see, instead of just point estimates, we now have uncertainty estimates over our model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pag7bEcmqkad"
      },
      "source": [
        "## 2.4 Model Evaluation: Model's Uncertainty\n",
        "We will sample different logistic regression lines to see how using a Bayesian approach can capture model undertainty.\n",
        "\n",
        "Here we again rely on Pyro's Predictive class, which allows for easy sampling of the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgYRa4h9qkad"
      },
      "outputs": [],
      "source": [
        "ax, grid, xx, yy = prepare_figure('Model evaluation')\n",
        "num_samples=10\n",
        "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=num_samples)\n",
        "svi_samples = predictive(grid, None)\n",
        "\n",
        "# Plot the mean decision surface \n",
        "logits = torch.mean(torch.mm(grid, torch.t(svi_samples['w'].squeeze())) + svi_samples['b'].squeeze(), axis=1).squeeze(-1)\n",
        "probs = Binomial(logits = logits).mean\n",
        "ax.contour(xx, yy, probs.reshape(xx.shape).detach().numpy(), levels=[.5], cmap=\"Reds\", vmin=0, vmax=1.5)\n",
        "\n",
        "# Sample and plot decision surfaces\n",
        "for i in range(num_samples):\n",
        "    logits = (torch.mm(grid, torch.t(svi_samples['w'][i,:])) + svi_samples['b'][i,:]).squeeze(-1)\n",
        "    probs = Binomial(logits = logits).mean\n",
        "    ax.contour(xx, yy, probs.reshape(xx.shape).detach().numpy(), levels=[.5], cmap=\"Greys\", vmin=0, vmax=1.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm7rm2Uhqkad"
      },
      "source": [
        "The above figure shows the uncertainty in our estimate of the logistic regression line. Note that for lower values of ruggedness there are many more data points, and as such, the regression lines are less uncertainty than for high ruggness values, where there is much more uncertainty. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CptrzXtXfyiG"
      },
      "source": [
        "## 2.5 The relationship between ruggedness and log GPD\n",
        "\n",
        "Finally, we can look about the uncertainty about the weights associated to Terrain Rugedness and logarithm of GDP. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOvNBT14qkad"
      },
      "source": [
        "Recall the learned parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maJ-Mxw6qkad"
      },
      "outputs": [],
      "source": [
        "for name, value in pyro.get_param_store().items():\n",
        "    print(name, pyro.param(name).data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrGPEIGLqkad"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "f, ax = plt.subplots(1, 2, figsize=(8, 6), sharex=True)\n",
        "for i in range(2):\n",
        "    mu = pyro.param('AutoNormal.locs.w')[0,i].data.numpy().squeeze()\n",
        "    std = pyro.param('AutoNormal.scales.w')[0,i].data.numpy().squeeze()\n",
        "    #x = np.linspace(mu - 3*std, mu + 3*std, 100)\n",
        "    x = np.linspace(-2,1, 100)\n",
        "    ax[i].plot(x, stats.norm.pdf(x, mu, std))\n",
        "ax[0].set_xlabel('Weight for ruggedness')\n",
        "ax[1].set_xlabel('Weight for log GDP')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpo6kGPRfyiL"
      },
      "source": [
        "### References\n",
        "  1. McElreath, D., *Statistical Rethinking, Chapter 7*, 2016\n",
        "  2. Nunn, N. & Puga, D., *[Ruggedness: The blessing of bad geography in Africa\"](https://diegopuga.org/papers/rugged.pdf)*, Review of Economics and Statistics 94(1), Feb. 2012"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "students_bayesian_logistic_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}